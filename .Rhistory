haberman$survival = factor(haberman$survival)
str(haberman)
h = glm(survival~age+op_year+no_nodes, data=haberman, family=binomial)
coef(h)
deviance(h)
new_patients=data.frame(age=c(37, 66), op_year=c(58, 60), no_nodes=c(5, 32))
predict(h, newdata=new_patients, type='responce')
predict(h, newdata=new_patients, type='response')
h = glm(survival~age+no_nodes, data=haberman, family=binomial)
coef(h)
deviance(h)
new_patients=data.frame(age=c(37, 66),  no_nodes=c(5, 32))
predict(h, newdata=new_patients, type='response')
ucla = read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
str(ucla)
library(ggplot2)
library(dplyr)
plot(ucla)
ucla %>% ggplot(aes(gre, admit)) + geom_point()
ucla %>% ggplot(aes(gre, admit)) + geom_jitter()
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=admit)
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=admit)
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=admit))
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=factor(admit))
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=factor(admit)))
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=factor(admit)))
ucla %>% ggplot(aes(gre, admit)) + geom_jitter(aes(col=factor(admit)), height = 0.1, width=0.0)
libra
install.packages(gridExtra)
library(gridExtra)
install.packages("gridExtra")
library(gridExtra)
p1 = ucla %>% ggplot(aes(gpa, admit))
p1 = ucla %>% ggplot(aes(gpa, admit)) + geom_jitter(aes(col=factor(admit)))
p1 = ucla %>% ggplot(aes(gpa, admit)) + geom_jitter(aes(col=factor(admit)))
p1 = ucla %>% ggplot(aes(gpa, admit)) + geom_jitter(aes(col=factor(admit)), height = 0.1, width= 0.0)
p2 = ucla %>% ggplot(aes(rank, admit))
p2 = ucla %>% ggplot(aes(rank, admit)) + geom_jitter(aes(col=factor(admit)), height = 0.1, width= 0.0)
grid.arrange(p1, p2, ncol=2)
m = glm(admit~., data=ucla, family=binomial)
coef(m)
deviance(m, type='responce')
source("~/workspace/R/ch08.R", echo=TRUE)
predict(m, new_data=s, type='responsive')
predict(m, new_data=s, type='response')
predict(m, new_data=s, type='response')
s = data.frame(gre=c(376), gpa=c(3.6), rank=c(3))
predict(m, new_data=s, type='response')
m = glm(admit~., data=ucla, family=binomial)
coef(m)
deviance(m, type='responce')
s = data.frame(gre=c(376), gpa=c(3.6), rank=c(3))
predict(m, new_data=s, type='response')
View(s)
m = glm(admit~., data=ucla, family=binomial)
s = data.frame(gre=c(376), gpa=c(3.6), rank=c(3))
predict(m, new_data=s, type='response')
install.packages("gridExtra")
library(gridExtra)
p1 = ucla %>% ggplot(aes(gpa, admit)) + geom_jitter(aes(col=factor(admit)), height = 0.1, width= 0.0)
p2 = ucla %>% ggplot(aes(rank, admit)) + geom_jitter(aes(col=factor(admit)), height = 0.1, width= 0.0)
grid.arrange(p1, p2, ncol=2)
m = glm(admit~., data=ucla, family=binomial)
coef(m)
deviance(m, type='responce')
s = data.frame(gre=c(376), gpa=c(3.6), rank=c(3))
predict(m, new_data=s, type='response')
m = glm(admit~., data=ucla, family=binomial)
# UCLA data
ucla = read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
m = glm(admit~., data=ucla, family=binomial)
s = data.frame(gre=c(376), gpa=c(3.6), rank=c(3))
predict(m, new_data=s, type='response')
library(survival)
str(colon)
?colon
m = glm(status~., data = colon, family = binomial)
m
table(is.na(colon))
clean_colon = na.omit(colon)
m
m = glm(status~., data = colon, family = binomial)
m
m = glm(status~., data = clean_colon, family = binomial)
m
install.libary("rpart")
package.install("rpart")
install.package("rpart")
install.packages("rpart")
library(rpart)
r = rpart(Species~., data=iris)
print(r)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(r, extra-1, digits=3)
rpart.plot(r, extra=1, digits=3)
rpart.plot(r, extra=1, digits=3)
pridict(r, iris, type='class')
predict(r, iris, type='class')
predict(r, iris, type='prob')
p = predict(r, iris, type='class')
table(p)
table(p, iris$SPecies)
table(p, iris$Species)
table(iris$Species, p)
library(randomForest)
install.packages("randomForest")
library(randomForest)
f = randomForest(Species~., data=iris)
f
plot(f)
varUsed(f)
varUsed(f)
varImpPlot(f)
treesize(f)
newd = data.frame(Sepal.Length = c(5.11, 7.01, 6.32), Sepal.Width=c(3.51, 3.2, 3.31), Petal.Length = c(1.4, 4.71, 6.02), Petal.Width = c(0.19, 1.4, 2.49))
predict(f, newdata=newd)
predict(f, newdata=newd, type = 'prob')
predict(f, newdata = newd, type='vote', norm.votes=FALSE)
predict(f, newdata = newd, type='vote', norm.votes=FALSE)
predict(f, newdata = newd, type='vote', norm.votes=FALSE)
small_forest=randomForest(Sprecies~., data=iris, ntree=20, nodesize=6, maxnodes=12)
treesize(small_forest)
small_forest=randomForest(Species~., data=iris, ntree=20, nodesize=6, maxnodes=12)
treesize(small_forest)
small_forest
install.packages("ㄷ1071")
install.packages("e1071")
s = svm(Species~., data=iris)
library(e1071)
s = svm(Species~., data=iris)
print(s)
table(predict(s, iris), iris$Species)
library(class)
train=iris
test=data.frame(Sepal.Length = c(5.11, 7.01, 6.32), Sepal.Width=c(3.51, 3.2, 3.31), Petal.Length = c(1.4, 4.71, 6.02), Petal.Width = c(0.19, 1.4, 2.49))
k = knn(train[,1:4], test, train$Species, k=5)
k
library(nnet, lib.loc = "/usr/local/Cellar/r/4.2.2/lib/R/library")
install.packages("devtools")
nn = nnet(Species~., iris, size=2)
library(devtools)
install.packages("devtools")
p = predict(nn, iris, type='class')
install.packages("devtools")
install.packages("devtools")
install.packages(RCurl)
install.packages(XML)
install.packages("RCurl")
install.packages("XML")
install.packages("tm")
install.packages("SnowballC")
gc()
clean_doc = xpathSApply(d, "//p", xmlValue)
t = readLines('https://en.wikipedia.org/wiki/Data_science')
d = htmlParse(t, asText=TRUE)
library(XML)
clean_doc = xpathSApply(d, "//p", xmlValue)
d = htmlParse(t, asText=TRUE)
clean_doc = xpathSApply(d, "//p", xmlValue)
clean_doc
clean_doc
doc = Corpus(VectorSource(clean_doc))
library(SnowballC)
library(tm)
doc = Corpus(VectorSource(clean_doc))
inspect(doc)
doc = tm_map(doc, content_transformer(tolower))
inspect(doc)
doc = tm_map(doc, removeNumbers)
doc = tm_map(doc, removeWords, stopwords('english'))
inspect(doc)
doc = tm_map(doc, removePunctuation)
doc = tm_map(doc, stripWhitespace)
inspect(doc)
dtm = DocumentTermMatrix(doc)
dim(dtm)
inspect(dtm)
function (x)
inspect(doc)
inspect(doc)
View(doc)
install.packages("wordcloud2")
install.packages("text2vec")
library(wordcloud2)
m = as.matrix(dtm)
v = sort(colSums(m), decreasing=T)
m = as.matrix(dtm)
m
v
wordcloud2(d)
d = data.frame(word = names(v), freq = v)
wordcloud2(d)
wordcloud2(d)
wordcloud2(d)
wordcloud2(d)
wordcloud2(d)
wordcloud2(d)
v
str(v)
summary(v)
dim(v)
str(v)
library(text2vec)
library(caret)
str(movie_review)
head(movie_review)
train_list = createDataPartition(y=movie_review$sentiment, p = 0.6, list=FALSE)
mtrain=movie_review[train_list, ]
mtest = movie_review[-train_list, ]
# 훈련 집합으로 DTM 구축
doc = Corpus(VectorSource(mtrain$review))
doc = tm_map(doc, content_transformer(tolower))
doc = tm_map(doc, removeNumbers)
doc = tm_map(doc, removeWords, stopwords('english'))
doc = tm_map(doc, removePunctuation)
doc = tm_map(doc, stripWhitespace)
dtm = DocumentTermMatrix(doc)
dim(dtm)
inspect(dtm)
dtm_small = removeSparseTerms(dtm, 0.90)
dim(dtm_small)
inspect(dtm_small)
X = as.matrix(dtm_small)
dataTrain = as.data.frame(cbind(mtrain$sentiment), X)
dataTrain$V1 = as.factor(dataTrain$V1)
colnames(dataTrain)[1] = 'y'
dataTrain
dataTrain = as.data.frame(cbind(mtrain$sentiment, X))
dataTrain$V1 = as.factor(dataTrain$V1)
colnames(dataTrain)[1] = 'y'
dataTrain
library(rpart)
r = rpart(y~., dataTrain)
printcp(r)
libray(rpart.plot)
library(rpart.plot)
rpart.plot(r, extra=1)
library(randomForest)
f = randomForest(y~., dataTrain)
f
docTest = Corpus(VectorSource(mtest$review))
docTest = tm_map(docTest, content_transformer(tolower))
docTest = tm_map(docTest, removeNumbers)
docTest = tm_map(docTest, removeWords, stopwords('english'))
docTest = tm_map(docTest, removePunctuation)
docTest = tm_map(docTest, stripWhitespace)
dtmTest = DocumentTermMatrix(docTest, control = list(dictionary=dtm_small$dimnames$Terms))
dim(dtmTest)
str(dtmTest)
inspect(dtmTest)
X = as.matrix(dtmTest)
X = as.matrix(dtmTest)
dataTest = as.data.frame(cbind(mtest$sentiment, X))
dataTest$V1 = as.factor(dataTest$V1)
colnames(dataTest)[1] = 'y'
pr =predict(r, newdata=dataTest, type='class')
table(pr, dataTest$y)
pf=predict(f, newdata=dataTest)
table(pf, dataTest$y)
resmp = resamples(list(선형=m, 일반화선형=g, 결정트리=r, 랜덤포리스트=f, SVM=s))
youtube = read.csv("https://www.kaggle.com/datasets/advaypatil/youtube-statistics?select=videos-stats.csv")
str(youtube)
# 0. 데이터 읽기 및 간단한 전처리 수행
wisconsin = read.csv("wisconsin.csv")
# 0. 데이터 읽기 및 간단한 전처리 수행
setwd("~/workspace/R")
wisconsin = read.csv("wisconsin.csv")
wisconsin = read.csv("wisconsin.csv")
str(wisconsin)
View(wisconsin)
summary(wisconsin)
# wjscjfl
table(is.na(wisconsin))
clean_wisconsin = na.omit(wisconsin) # 결측치 처리
table(is.na(clean_wisconsin))
clean_wisconsin$Diagnosis = factor(clean_wisconsin$Diagnosis) # 반응변수 Diagnosis 범주화
str(clean_wisconsin$Diagnosis)
clean_wisconsin = na.omit(wisconsin) # 결측치 처리
table(is.na(clean_wisconsin))
clean_wisconsin$Diagnosis = as.factor(clean_wisconsin$Diagnosis) # 반응변수 Diagnosis 범주화
str(clean_wisconsin$Diagnosis)
clean_wisconsin %>%
ggplot(aes(Radius_mean, Diagnosis)) +
geom_jitter(aes(col=Diagnosis), height=0.3, width=0.0) +
geom_boxplot(alpha=0)
# Diagnosis와 설명변수(Radius_mean, Radius_se, Radius_extreme, Area_mean, Area_se, Area_extreme)와의 상관관계를 그래프를 그려서 분석
library(dplyr)
library(ggplot2)
clean_wisconsin %>%
ggplot(aes(Radius_mean, Diagnosis)) +
geom_jitter(aes(col=Diagnosis), height=0.3, width=0.0) +
geom_boxplot(alpha=0)
# Radius_se
clean_wisconsin %>%
ggplot(aes(Radius_se, Diagnosis)) +
geom_jitter(aes(col=Diagnosis)) +
geom_boxplot(alpha = 0)
# Radius_se
clean_wisconsin %>%
ggplot(aes(Radius_se, Diagnosis)) +
geom_jitter(aes(col=Diagnosis), height=0.3, width=0.0) +
geom_boxplot(alpha = 0)
clean_wisconsin %>%
ggplot(aes(Radius_mean, Diagnosis)) +
geom_jitter(aes(col=Diagnosis), height=0.3, width=0.0) +
geom_boxplot(alpha=0.3)
# Radius_se
clean_wisconsin %>%
ggplot(aes(Radius_se, Diagnosis)) +
geom_jitter(aes(col=Diagnosis), height=0.3, width=0.0) +
geom_boxplot(alpha = 0.3)
# Area_extreme
clean_wisconsin %>%
ggplot(aes(Area_extreme, Diagnosis)) +
geom_jitter(aes(col=Diagnosis), height=0.3, width=0.0) +
geom_boxplot(alpha = 0.3)
View(clean_wisconsin)
# 2. 결정트리 및 랜덤포리스트 모델 훈련(rpart, randomForest 함수 이용) 및 분석
select_wisconsin = clean_wisconsin %>%
select(-1)
View(clean_wisconsin)
View(select_wisconsin)
# 결정트리 모델 훈련 및 분석
library(rpart)
r = rpart(Diagnosis~., data=select_wisconsin)
print(r)
# 모델 분석
library(rpart.plot)
rpart.plot(r, extra=1, digits=3)
rpart.plot(r, extra=1)
, digits=3
rpart.plot(r, extra=1, digits=3)
rpart.plot(r, extra=1)
printcp(r)
r_pred = predict(r, select_wisconsin, type='class') # 훈련집합으로 예측 및 혼동 행렬 작성
table(r_pred, select_wisconsin$Diagnosis)
summary(r)
f = randomForest(Diagnosis~., data=select_wisconsin)
print(f)
# 랜덤포리스트 모델 훈련 및 분석
library(randomForest)
f = randomForest(Diagnosis~., data=select_wisconsin)
print(f)
# 모델 분석
plot(f)
varUsed(f)
varImpPlot(f)
treesize(f)
f_pred = predict(f, select_wisconsin) # 훈련집합으로 예측 및 혼동 행렬 작성
table(f_pred, select_wisconsin$Diagnosis)
# 3. 여러 모델의 성능비교(caret 패키지 이용)
library(caret)
control = trainControl(method='cv', number=5)
r = train(Diagnosis~., data = select_wisconsin, method = 'rpart', metric = 'Accuracy', trControl = control)
f = train(Diagnosis~., data = select_wisconsin, method = 'rf', metric = 'Accuracy', trControl = control)
k = train(Diagnosis~., data = select_wisconsin, method = 'knn', metric = 'Accuracy', trControl = control)
s = train(Diagnosis~., data = select_wisconsin, method = 'svmRadial', metric = 'Accuracy', trControl = control)
g = train(Diagnosis~., data = select_wisconsin, method = 'glm', family = binomial, metric = 'Accuracy', trControl = control)
resamp = resamples(list(결정트리 = r, 랜덤포리스트 = f, SVM = s, kNN = k, GLM = g))
summary(resamp)
sort(resamp, decreasing = T)
dotplot(resamp)
library(stringr)
setwd("~/workspace/R")
x = read.csv('googleplaystore.csv', fileEncoding = 'utf-8')
str(x)
View(x)
x = x[-10473, ]
# Size열에서 값 대신 "Varies with device"로 표기된 데이터를 NA로 표시함
x$Size = sub("Varies with device", NA, x$Size)
x$Size = sub("M", "e6", x$Size, fixed = TRUE)
x$Size = sub("k", "e3", x$Size, fixed = TRUE)
x$Size = as.numeric(x$Size)
# Installs 열에 기록된 숫자보다 더 크다는 의미의 + 기호가 포함된 관측값과
# 1,000 단위 숫자 구분에 사용되는 ,가 포함된 관측값이 있으므로
# str_replace, str_replace_all 함수를 사용해 빈 문자로 치환하여 처리함
x$Installs = str_replace(x$Installs, '\\+', '')
x$Installs = str_replace_all(x$Installs, ',', '')
x$Installs = as.numeric(x$Installs)
# Price에 포함된 $ 기호를 제거하고, 숫자형으로 변환
x$Price = str_replace(x$Price, "\\$", '')
x$Price = as.numeric(x$Price)
x = na.omit(x)
View(x)
# 각 측정 항목의 데이터형을 고려하여 나머지 변수의 형 변환을 실행함
x$Category = factor(x$Category)
x$Reviews = as.numeric(x$Reviews)
x$Type = factor(x$Type)
x$Content.Rating = factor(x$Content.Rating)
x$Genres = factor(x$Genres)
# 마지막 Date형의 손쉬운 변환을 위해 lubridate 라이브러리의 mdy 함수를 이용할 수 있음
library(lubridate)
x$Last.Updated = mdy(x$Last.Updated)
# 평점분포
library(ggplot2)
glimpse(x)
# histogram - sample 수가 차이나 비교가 힘듦
x %>% ggplot(aes(Rating, fill=Type)) + geom_histogram(position='dodge')
# 평점분포
library(ggplot2)
x %>% ggplot(aes(Rating)) + geom_histogram( )
# histogram - sample 수가 차이나 비교가 힘듦
x %>% ggplot(aes(Rating, fill=Type)) + geom_histogram(position='dodge')
# density - sample 수가 아닌 비율로 비교
x %>% ggplot(aes(Rating, col = Type)) + geom_density()
# 평점과 리뷰의 관계
x %>% ggplot(aes(Reviews, Rating, col = Type)) + geom_point(alpha = 0.2) + scale_x_log10()
# 평점과 리뷰, 설치 횟수의 관계
x %>% ggplot(aes(Reviews, Rating, col = Type, size = Installs)) + geom_point(alpha = 0.2) + scale_x_log10()
x %>% ggplot(aes(Reviews, Rating, size = Installs)) + geom_point(alpha = 0.2) + scale_x_log10() + facet_wrap(~Type)
x %>% filter(Type == "Paid") %>% ggplot(aes(Reviews, Rating, size = Installs)) + geom_point(alpha = 0.2) + scale_x_log10()
# 리뷰~설치 횟수
x %>% ggplot(aes(Reviews, Installs)) + geom_point(alpha = 0.2) + scale_x_log10() + scale_y_log10() + geom_jitter()
# 리뷰~설치 횟수
x %>% ggplot(aes(Reviews, Installs)) + geom_point(alpha = 0.2) + scale_x_log10() + scale_y_log10()
x %>% ggplot(aes(Reviews, Installs)) + geom_point(alpha = 0.2) + scale_x_log10() + scale_y_log10() + geom_jitter()
# 평점~가격
x %>% filter(Type=="Paid" & Price < 5) %>% ggplot(aes(Price) + geom_histogram(binwidth = 0.01))
# 평점~가격
x %>% filter(Type=="Paid" & Price < 5) %>% ggplot(aes(Price) + geom_histogram(binwidth = 0.01))
# 평점~가격
x %>% filter(Type=="Paid" & Price < 5) %>% ggplot(aes(Price)) + geom_histogram(binwidth = 0.01))
# 평점~가격
x %>% filter(Type=="Paid" & Price < 5) %>% ggplot(aes(Price)) + geom_histogram(binwidth = 0.01)
x %>% filter(Type=="Paid" & Price > 5 & Price < 10) %>% ggplot(aes(Price)) + geom_histogram(binwidth = 0.01)
x %>% filter(Type == "Paid") %>% ggplot(aes(Price, Rating)) + geom_point(alpha = 0.2)
x %>% filter(Type == "Paid") %>% ggplot(aes(Price, Rating)) + geom_point(alpha = 0.2) + scale_x_log10()
x %>% filter(Type == "Paid" & Price < 50) %>% ggplot(aes(Price, Rating)) + geom_point(alpha = 0.2)
top4 = x %>% group_by(Category) %>% summarize(n = n()) %>% arrange(desc(n)) %>% head(4)
top4
x %>% filter(Type == "Paid" & Price < 50  & Category %in% top4$Category) %>%
ggplot(aes(Price, Rating)) + geom_point(alpha = 0.2) + facet_wrap(~Category)
# 평점 ~ 크기
x %>% ggplot(aes(Size, Rating)) + geom_point(alpha = 0.1)
x %>% ggplot(aes(Content.Rating, Rating)) + geom_point(alpha = 0.1) + geom_jitter( )
x %>% ggplot(aes(Rating, col=Content.Rating)) + geom_density()
x %>% filter(Content.Rating!="Adults only 18+") %>% ggplot(aes(Rating, col=Content.Rating)) + geom_density()
# 평점 ~ 카테고리
x %>% ggplot(aes(Category, Rating)) + geom_point(size = 0.1, position= "jitter") + coord_flip()
# 다중 선형 회귀 모델
m = lm(Rating~Size+Content.Rating+Category, data = x)
m
deviance(m)/nrow(x)
# 다중 선형 회귀 모델
m = lm(Rating~Size+Content.Rating+Category, data = x)
m
deviance(m)/nrow(x)
library(caret)
ps = select(x, Rating, Category, Size, Content.Rating)
control = trainControl(method='cv', number = 10)
m = train(Rating~., data=ps, method='lm', metric='RMSE', trControl = control)
m
m$resample
m$results
ps = select(x, Rating, Category, Size)
control = trainControl(method = 'cv', number = 10)
m = train(Rating~., data=ps, method='lm', metric='RMSE', trControl = control)
g = train(Rating~., data=ps, method='glm', metric='RMSE', trControl = control)
r = train(Rating~., data=ps, method='rpart', metric='RMSE', trControl = control)
f = train(Rating~., data=ps, method='rf', metric='RMSE', trControl = control)
help(diamonds)
# 1. diamond 데이터셋은 ggplot2 패키지에 있는 데이터셋이다.
# dplyr와 ggplot2 패키지의 함수들을 이용하여 아래의 분석과제를 수행하는 R코드를 작성하라.
library(ggplot2)
help(diamonds)
help(diamond)
help(diamonds)
dia = diamonds
str(dia)
summaary(dia)
summary(dia)
dia %>% ggplot(aes(carat, price)) + geom_point()
dia %>% ggplot(aes(carat, price)) + geom_point()
dia %>% ggplot(aes(carat, price, col = clarity)) + geom_point()
dia %>% ggplot(aes(carat, price, col = cut)) + geom_point()
dia %>% ggplot(aes(carat, price, col = color)) + geom_point()
dia %>% ggplot(aes(carat, price, col = color)) + geom_point() + geom_smooth()
# 2-1) 일강수량을 예측하는 회귀모델링을 진행한 후, 적합된 모델의 회귀식을 쓰시오.
library(scatterplot3d)
x = c(3.0, 6.0, 3.0, 6.0)
u = c(10, 10, 20, 20)
y = c(4.65, 5.9, 6.7, 8.02)
scatterplot3d(x, u, y, xlim=2:7, ylim=7:23, zlim=0:10, pch=20, type='h')
m = lm(y~x,u)
m = lm(y~x+u)
coef(m)
# 2-2) 수립된 회귀모델의 통계량을 summary() 함수로 출력하여 간략히 분석하라.
summary(m)
scatterplot3d(trees$Girth, trees$Height, trees$Volume)
scatterplot3d(trees$Girth, trees$Height, trees$Volume)
m = lm(Volume ~ Girth + Height, data=trees)
coef(m)
m
# 2-2) 수립된 회귀모델의 통계량을 summary() 함수로 출력하여 간략히 분석하라.
summary(m)
residuals(m)
# 3-2) 구축된 랜덤포리스트 모델에서 중요한 변수들은 어떤 것들인지 살펴보고, 그 결과를 간단히 기술하라.
varUsed(f)
# 3-1) 합격여부(admit)를 에측하는 랜덤포리스트 모델을 수립한 후, 결과의 OOB 데이터로부터 나온 혼동행렬을 기술하고 정확률을 구하라.
library(randomForest)
f = randomForest(admit~., data=ucla)
# 3. UCLA admission 데이터셋을 읽어들인 후 아래 모델링을 수행하라.
ucla = read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
ucla$admit = factor(ucla$admit)
f = randomForest(admit~., data=ucla)
f
# 3-2) 구축된 랜덤포리스트 모델에서 중요한 변수들은 어떤 것들인지 살펴보고, 그 결과를 간단히 기술하라.
varUsed(f)
# 두 번째 설명변수인 gpa가 질문에 가장 많이 사용되었고, 세 번째 설명변수 rank가 질문에 가장 적게 사용되었음
varImpPlot(f)
